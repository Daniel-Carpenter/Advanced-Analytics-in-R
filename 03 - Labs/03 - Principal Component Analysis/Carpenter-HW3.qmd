---
title: "Homework 3 - Principal Component Analysis"
author: "Daniel Carpenter"
date: "August 2022"
format: 
  pdf:
    toc: true
    toc-depth: 2
    # number-sections: true
    highlight-style: github #arrow
    code-block-border-left: "#D6D6D6"
  gfm:
    toc: true
    toc-depth: 1
    
execution:
  echo:    true
  include: true
  cache:   true
  message: false
  warning: false

editor: visual

fig-width:  7
fig-height: 3.5
---


***Check list:***

* 1 (iv - v)

\newpage

## Packages

```{r, message=FALSE}
library(tidyverse) # get tidverse for piping
library(skimr)
library(knitr)
library(scales)
require(lubridate)

library(mlbench)       # Glass data
library(ggbiplot)      # biplots
library(corrplot)
library(caret)
```

# 1. Glass Data

## Get and Clean Data
```{r}
data(Glass)

# Remove duplicates
Glass <- Glass[!duplicated(Glass), ]
```

## (a) Mathematics of PCA

`i.` Create the correlation matrix of all the numerical attributes in the `Glass` data and store the results in a new object `corMat`

```{r}
skimmed <- skim(Glass)

# Notice one factor data, for variable `type`
skimmed$skim_type

# Get only numeric data
GlassNumeric <- Glass %>% select(where(is.numeric))

# Create correlation matrix using only numeric data type
corMat <- cor(GlassNumeric)
```


`ii.` Compute the eigenvalues and eigenvectors of `corMat.`

Eigenvalues
```{r}
# prcomp(corMat)
eigenValues = eigen(corMat)$values
eigenValues
```

Eigenvectors
```{r}
eigenVectors = eigen(corMat)$vectors
eigenVectors
```


`iii.` Use `prcomp` to compute the principal components of the `Glass` attributes (make sure to use the scale option).
```{r}
# Using only numeric data
pc.glass <- prcomp(GlassNumeric, scale = TRUE)
pc.glass
```


`iv.` Compare the results from (ii) and (iii) - Are they the same? Different? Why? <br>

* The eigenvalues differ
* The eigenvectors are the same in absolute value, but the signs are the opposite within each value of the vectors
* Why do they differ? Past `ii` uses the correlation matrix; the principal component analysis (`ii`) uses the covariance matrix, which is a scaled, or *normalized*, version of the correlation matrix. 


`v.` Using R demonstrate that principal components 1 and 2 from (iii) are orthogonal. (Hint: the inner product between two    vectors is useful in determining the angle between the two vectors)

```{r}
PC1.glass <- pc.glass$x[,1]
PC2.glass <- pc.glass$x[,2]

angle <- acos( sum(PC1.glass*PC2.glass) / ( sqrt(sum(PC1.glass * PC1.glass)) * sqrt(sum(PC2.glass * PC2.glass)) ) )

angle

```


\newpage

## (b) Applications of PCA

i. Create a visualization of the corMat correlation matrix (i.e., a heatmap or variant).

* [`corrplot` options](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html).

```{r}
testRes = cor.mtest(GlassNumeric, conf.level = 0.90)


# Correlation matrix to show spread and significance
corrplot(corMat, 
         p.mat     = testRes$p, # Significance 'x' marks 
         sig.level = 0.10,      # ""           levels
         order     = 'hclust',  # Clustering
         addrect   = 2,
         method    = 'ellipse') # Show spread and direction        

```

\newpage

ii. Provide visualizations of the principal component analysis results from the Glass data. Consider incorporating the glass type to group and color your biplot.

```{r, fig.height=6}

# First show the spread of the components
plot(pc.glass,
     main = 'Principal Components Explanation of Data',
     xlab = 'Principal Components',
     col  = 'lightsteelblue3'
     )
```

```{r, fig.width=7, fig.height=5}
# NExt show the biplots
ggbiplot(pc.glass,
         obs.scale    = 1, 
         var.scale    = 1, 
         varname.size = 4, 
         labels.size  = 10, 
         circle       = TRUE,
         group        = Glass$Type#,
         # ellipse      = TRUE
         ) +
  
  # Titles and caption
  labs(title   = 'Representativeness of First Two Principal Components',
       caption = '\nUsing Glass data from mlbench') +
  
  # Add color to points by glass type
  geom_point(aes(colour=Glass$Type), size = 1) +
  
  # Categorical palette on glass type
  scale_color_brewer(name = 'Glass Type', 
                     palette = 'Set2', type = 'qual') + 
  
  theme_minimal() # the theme

```


iii. Provide an interpretation of the first two prinicpal components the Glass data.

* Both PC1 and PC2 represent roughly half (50%) of the cumulative proportion of variance (see summary below)

* PC1 best explains `Fe`, `K`, and `Si` glass types, since they lie closest to parallel with the x axis

* PC2 best represents `Ba`, and `Mg`, since they lie close to parallel with the y axis.

* Other variables appear to be explained by both principal components, since they are near a 45 degree angle.


Summary of cumulative proportion located here
```{r}
summary(pc.glass)
```


iv. Based on the PCA results, do you believe that you can effectively reduce the dimension of the data? If so, to what degree? If not, why?

* Given the cumulative proportions above, it is clear that the first two principal components capture only half (~50%) of the variation in the original data. We could compare that to a coin flip, or a random chance.

* However, the *first four* PC's capture roughly 80%. This cuts the number of variables in half, which is impressive. 

* Note that if your `q` threshold was set to 95%, then this analysis would not perform well, since all but one of the PC's capture 95% of the variation in the actual data.


\newpage

## (c) Application of LDA

i. Since the Glass data is grouped into various labeled glass types we can consider linear discriminant analysis (LDA) as another form of dimension reduction. Use the lda method from the MASS package to reduce the Glass data dimensionality.


ii. How would you interpret the first discriminant function, LD1?


iii. Use the ldahist function from the MASS package to visualize the results for LD1 and LD2.
Comment on the results. 


\newpage

# 2. Principal components for dimension reduction


\newpage

# 3. Housing data dimension reduction and exploration
















