---
title: "Homework 3 - Principal Component Analysis"
author: "Daniel Carpenter & Zachary Knepp"
date: "August 2022"
format: 
  pdf:
    toc: true
    toc-depth: 2
    # number-sections: true
    highlight-style: github #arrow
    code-block-border-left: "#D6D6D6"
  gfm:
    toc: true
    toc-depth: 2
    
execution:
  echo:    true
  include: true
  cache:   true
  message: false
  warning: false

editor: visual

fig-width:  7
fig-height: 3.5
---

\newpage

## Packages

```{r, message=FALSE}
library(tidyverse) # get tidverse for piping
library(skimr)
library(knitr)
library(scales)
require(lubridate)

library(mlbench)  # Glass data
library(ggbiplot) # biplots
library(corrplot)
library(caret)    # preProcess for centering and z score scaling 
library(MASS)     # Linear discreiminant analysis with lda 

library(HSAUR2)   # olympic data
library(outliers) # grubbs.test
library(DescTools) # for the %like% operator
```

# 1. Glass Data

## Get and Clean Data

```{r}
data(Glass)

# Remove duplicates
Glass <- Glass[!duplicated(Glass), ]
```

## 1 (a) Mathematics of PCA

`i.` Create the correlation matrix of all the numerical attributes in the `Glass` data and store the results in a new object `corMat`

```{r}
skimmed <- skim(Glass)

# Notice one factor data, for variable `type`
skimmed$skim_type

# Get only numeric data
GlassNumeric <- Glass %>% dplyr::select(where(is.numeric))

# Create correlation matrix using only numeric data type
corMat <- cor(GlassNumeric)
```

`ii.` Compute the eigenvalues and eigenvectors of `corMat.`

Eigenvalues

```{r}
# prcomp(corMat)
eigenValues = eigen(corMat)$values
eigenValues
```

Eigenvectors

```{r}
eigenVectors = eigen(corMat)$vectors
eigenVectors
```

`iii.` Use `prcomp` to compute the principal components of the `Glass` attributes (make sure to use the scale option).

```{r}
# Using only numeric data
pc.glass <- prcomp(GlassNumeric, scale = TRUE)
pc.glass
```

`iv.` Compare the results from (ii) and (iii) - Are they the same? Different? Why? <br>

-   The eigenvalues differ
-   The eigenvectors are the same in absolute value, but the signs are the opposite within each value of the vectors
-   Why do they differ? Past `ii` uses the correlation matrix; the principal component analysis (`ii`) uses the covariance matrix, which is a scaled, or *normalized*, version of the correlation matrix.

`v.` Using R demonstrate that principal components 1 and 2 from (iii) are orthogonal. (Hint: the inner product between two vectors is useful in determining the angle between the two vectors)

-   The dot product the two vectors is nearly 0, so we can assume orthoginal

```{r}
df.pc.glass <- data.frame(pc.glass$x)

#Orthogonal because they dot product is practically 0
df.pc.glass$PC2 %*% df.pc.glass$PC1


```

\newpage

## 1 (b) Applications of PCA

i.  Create a visualization of the corMat correlation matrix (i.e., a heatmap or variant).

-   [`corrplot` options](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html).

```{r}
testRes = cor.mtest(GlassNumeric, conf.level = 0.90)


# Correlation matrix to show spread and significance
corrplot(corMat, 
         p.mat     = testRes$p, # Significance 'x' marks 
         sig.level = 0.10,      # ""           levels
         order     = 'hclust',  # Clustering
         addrect   = 2,
         method    = 'ellipse') # Show spread and direction        

```

\newpage

ii. Provide visualizations of the principal component analysis results from the Glass data. Consider incorporating the glass type to group and color your biplot.

```{r, fig.height=6}

# First show the spread of the components
plot(pc.glass,
     main = 'Principal Components Explanation of Data',
     xlab = 'Principal Components',
     col  = 'lightsteelblue3'
     )
```

```{r, fig.width=7, fig.height=5}
# NExt show the biplots
ggbiplot(pc.glass,
         obs.scale    = 1, 
         var.scale    = 1, 
         varname.size = 4, 
         labels.size  = 10, 
         circle       = TRUE,
         group        = Glass$Type#,
         # ellipse      = TRUE
         ) +
  
  # Titles and caption
  labs(title   = 'Representativeness of First Two Principal Components',
       caption = '\nUsing Glass data from mlbench') +
  
  # Add color to points by glass type
  geom_point(aes(colour=Glass$Type), size = 1) +
  
  # Categorical palette on glass type
  scale_color_brewer(name = 'Glass Type', 
                     palette = 'Set2', type = 'qual') + 
  
  theme_minimal() # the theme

```

iii. Provide an interpretation of the first two prinicpal components the Glass data.

-   Both PC1 and PC2 represent roughly half (50%) of the cumulative proportion of variance (see summary below)

-   PC1 best explains `Fe`, `K`, and `Si` glass types, since they lie closest to parallel with the x axis

-   PC2 best represents `Ba`, and `Mg`, since they lie close to parallel with the y axis.

-   Other variables appear to be explained by both principal components, since they are near a 45 degree angle.

Summary of cumulative proportion located here

```{r}
summary(pc.glass)
```

iv. Based on the PCA results, do you believe that you can effectively reduce the dimension of the data? If so, to what degree? If not, why?

-   Given the cumulative proportions above, it is clear that the first two principal components capture only half (\~50%) of the variation in the original data. We could compare that to a coin flip, or a random chance.

-   However, the *first four* PC's capture roughly 80%. This cuts the number of variables in half, which is impressive.

-   Note that if your `q` threshold was set to 95%, then this analysis would not perform well, since all but one of the PC's capture 95% of the variation in the actual data.

\newpage

## 1 (c) Application of LDA

i.  Since the `Glass` data is grouped into various labeled glass types we can consider linear discriminant analysis (LDA) as another form of dimension reduction. Use the `lda` method from the `MASS` package to reduce the `Glass` data dimensionality.

```{r}
# Prep type for the lda hist
GlassNumType<- Glass %>% mutate(Type = as.numeric(as.factor(Type)))

preproc.param <- Glass %>% preProcess(method = c("center", "scale")) 

# Transform the data using the estimated parameters 
# Apply centering and scaling to the dataset
transformed <- preproc.param %>% predict(Glass)

# Fit the model , '.' means *
lda.model <- lda(Type ~ ., data = transformed) 
lda.model
```

ii. How would you interpret the first discriminant function, LD1?

-   Within the coefficient matric, you can see that LD1's top 3 coefficients contain Na, Si, and Al, which shows that there are greater levels of separation

-   Additionally, LD1 holds 81% of the discriminitory power. This values holds an important role in between group discrimination.

iii. Use the ldahist function from the MASS package to visualize the results for LD1 and LD2. Comment on the results.

-   You can see that there is some spread from 0 between the first 3 graphs, and the last 3 graphs. The first graphs are on the negative side of 0, where the bottom 3 are primarily on the positive side of 0.

```{r, fig.height=4}
# ------------

# Predict the values
predictions <- predict(lda.model) 

# Create the LDA histogram
par(mar=c(1,1,1,1))
ldahist(predictions$x[ , 1], GlassNumType$Type)
```

\newpage

# 2. Principal components for dimension reduction

## 2 (a) Remove outlier

-   Launa from PNG is the outlier. She is an outlier in highjump, longjump, run800m, and hurdles. See image below.

-   Notice that Joyner is an outlier too, but not removed since problem does not specify

```{r}
data(heptathlon)

grubbs.test(heptathlon$score)

# Who is the outlier?
heptathlon[heptathlon$score==outlier(heptathlon$score),]


# To show the outlier
heptathlonPivot <- heptathlon %>%

  # Get the competitor name as own col
  mutate(competitor = rownames(heptathlon) )  %>%
  
  pivot_longer(cols      = hurdles:score,
               names_to  = 'event',
               values_to = 'eventScore')

# Create a plot of each event
heptathlonPivot %>%
  
  ggplot(aes(x = 1,
             y = eventScore)) +

  # To see the distribution
  geom_boxplot(color = 'steelblue4', fill = 'lightsteelblue') +
  
  # Display the olympians
  geom_text(label = heptathlonPivot$competitor, size = 1.5) +
  
  facet_wrap(. ~ event, nrow = 1, scales = 'free') + 
  
  # Aesthetics
  ggtitle('Distribution of Event Scores from the Heptathlon') +
  theme_minimal() + theme(axis.text.x  = element_blank(),
                          axis.title.x = element_blank(),
                          panel.grid.major.x = element_blank(),
                          panel.grid.minor.x = element_blank())


# Remove outlier from data set
heptathlon <- heptathlon[heptathlon$score!=outlier(heptathlon$score),]

```

## 2 (b)

### Transform the running events (hurdles, run200m, run800m) so that large values are good.

```{r}
heptathlon.goodBad <- heptathlon %>%
  mutate(hurdles = max(hurdles) - hurdles, 
         run200m = max(run200m) - run200m, 
         run800m = max(run800m) - run800m 
         )
```

## 2 (c)

### Perform PCA and store in `Hpca`

```{r}
Hpca <- prcomp(heptathlon.goodBad,
               center = TRUE, # Mean centered  
               scale  = TRUE  # Z-SCore standardized
               )
```

## 2 (d)

### Visualize first two principal components

```{r}
# Create the biplot
ggbiplot(Hpca,
         obs.scale    = 1, 
         var.scale    = 1, 
         varname.size = 4, 
         labels.size  = 10, 
         circle       = TRUE
         ) +
  
  # Titles and caption
  labs(title   = 'Representativeness of First Two Principal Components',
       caption = '\nUsing scaled and centered heptathlon data from HSAUR2') +
  
  # Add color to points by glass type
  geom_point(color = 'darkseagreen3', size = 2) +
  
  theme_minimal() # the theme

```

### Interpretation of Results

#### `PC1`:

-   Explains 66% of the variation in the data

-   Explains all variables very well *(except for `run800m`, `javelin`, and `highjump`)*

#### `PC2`:

-   Explains 11% of the variation in the data

-   Explains some of `run800m`, `javelin`, and `highjump`. Since the angles are nearly 45 degrees, you can tell that the explanatory power splits between `PC1` and `PC2`

\newpage

## 2 (e)

Plot the heptathlon score against the principal component 1 projections

```{r}

heptathlon %>%
  
  # Add PC1 to dataset
  mutate(PC1 = Hpca$x[, 1]) %>%
  
  # Using PC1 and score
  ggplot(aes(y = score, 
             x = PC1
             ) ) +
    
    # Add OLS
    geom_smooth(method = lm, color = 'grey80', alpha = 0.10) +
  
    # Add points
    geom_point(color = 'steelblue3',
               alpha = 0.5, 
               size  = 5) +
  
    # Labels and colors
    labs(
       title = 'How well does PC1 explain the competitor score',
       y = 'Competor Score'
    ) +
    
    scale_y_continuous(labels = comma) + 
    
    theme_minimal()
```

### Interpretation of Score \~ PC1

-   You can see that PC1 explains the competitor score extremely well just by this simple visual.

\newpage

# 3. Housing data dimension reduction and exploration

## Read and Clean data

```{r}
# Read data
housingData <- read_csv('housingData.csv')


hd <- housingData %>%
  
  # selects only numeric columns
  select_if(is.numeric) %>%
  
  # creates new variables age, ageSinceRemodel, and ageofGarage, and
  dplyr::mutate(age = YrSold - YearBuilt,
                ageSinceRemodel = YrSold - YearRemodAdd,
                ageofGarage = ifelse(is.na(GarageYrBlt), age, YrSold - GarageYrBlt)) %>%
  
  # removes a few columns that are not needed
  dplyr::select(!c(Id,MSSubClass, LotFrontage, GarageYrBlt,
                   MiscVal, YrSold , MoSold, YearBuilt,
                   YearRemodAdd, MasVnrArea))



```

## Correlation Analysis

Please see interpretation of correlation analysis below:

-   You can see that there are many variables within the data set that are highly correlated

-   Some variables do not show high correlation

-   Please see correlation plot in depth for variable specific correlations.

```{r, fig.height=8, fig.width=8}
# Parameters
numericData = hd
confInt     = 0.90

# Create correlation matrix
corMatHouse <- cor(numericData)

testRes = cor.mtest(numericData, conf.level = confInt)


# Correlation matrix to show spread and significance
corrplot(corMatHouse, 
         order     = 'hclust',  # Clustering
         addrect   = 2,
         method    = 'ellipse', # Show spread and direction  
         type      = 'lower')   
```

\newpage

## Principal Component Analysis

### Cumulative Proportions

Please see interpretation of cumulative proportions below:

-   The first two principal components explain \~36% of the variation in the data, so we would need to use more than just those

-   If our `q` threshold for variation kept within the data was arounf 95%, then we could keep only the first 20 principal components while removing 10 columns from the original data set.

-   If we were satisfied with retaining 80% of the variation via these PC's, we could remove 18 variables from the data set.

-   Note that there are 30 numeric variables within the dataset

```{r}
ncol(hd) # Number of columns in the dataset

# Perform PCA while centering and scaling data
pc.house <- prcomp(hd,
                   center = TRUE, # Mean centered  
                   scale  = TRUE  # Z-SCore standardized
                   )

# See first 10 cumulative proportions
pc.house.summary <- summary(pc.house)
pc.house.summary$importance[, 1:6]

# Create a data frame for ggplot
pc.hd.cumProportion <- data.frame(cumProp = pc.house.summary$importance['Cumulative Proportion', ])

# See how Individual PC's perform
pc.hd.cumProportion %>%
  
  # Create the plot with x summarizing the PC's as only their number
  ggplot(aes(x = reorder(1:nrow(pc.hd.cumProportion), cumProp),
             group = 1,
             y = cumProp)
         ) +
  
  # Area chart
  geom_area(alpha = 0.5,
            color = 'steelblue3',
            fill  = 'lightsteelblue1'
            ) +
  
  # Labels, scales, and colors
  labs(title   = 'Cumulative Proportion of Principal Components',
       x       = 'Principal Component',
       y       = 'Cumulative Proportion of Explanatory Power',
       caption = '\nUsing housing dataset. Data centered and scaled.'
       ) +
  
  scale_y_continuous(labels = percent) +
  
  theme_minimal() + theme(panel.grid.major.x = element_line(color = 'grey95'),
                          panel.grid.major.y = element_line(color = 'grey65'))

```

\newpage

### Bi-Plots

-   Since there are so many variables, it can be difficult to interpret all of the results

-   However, you can see that there are not many outliers affecting the analysis

-   Some variables that PC1 contains well: `Fireplaces`, `OverallQual`, `GarageCars.`

-   Some variables that PC2 contains well: `BsmtFullBath`, `BsmtFinSF1`, `EncPorchSF`

```{r, fig.width=8, fig.height=4}
ggbiplot(pc.house,
         obs.scale    = 1, 
         var.scale    = 1, 
         varname.size = 3, 
         labels.size  = 10, 
         circle       = TRUE,
         alpha        = 0.1
         ) +
  
  # Titles and caption
  labs(title   = 'Representativeness of First Two Principal Components',
       caption = '\nUsing housing dataset. Data centered and scaled.') +
  
  theme_minimal() # the theme
```
