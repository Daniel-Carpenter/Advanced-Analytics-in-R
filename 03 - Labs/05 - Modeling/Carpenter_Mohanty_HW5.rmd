---
title:    "ISE 5103 Intelligent Data Analytics"
subtitle: "Homework 5 - Modeling"
author:   "Daniel Carpenter & Sonaxy Mohanty"
date:     "October 2022"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    highlight: arrow
  # github_document:
  #   toc: yes
  #   toc_depth: 2
urlcolor: blue
cache: true
fig.width: 7
fig.height: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

## Packages

```{r error=FALSE, message=FALSE, warning=FALSE}
# Data Wrangling
library(tidyverse)

# Modeling
library(MASS)
library(caret) # Modeling variants like SVM
library(earth) # Modeling with Mars

# Aesthetics
library(knitr)
library(cowplot)  # multiple ggplots on one plot with plot_grid()
library(scales)
library(kableExtra)
```

## General Data Prep

### Read Data
```{r}
hd <- read.csv('housingData.csv') %>%
  
  # creates new variables age, ageSinceRemodel, and ageofGarage, and
  dplyr::mutate(age = YrSold - YearBuilt,
                ageSinceRemodel = YrSold - YearRemodAdd,
                ageofGarage = ifelse(is.na(GarageYrBlt), age, YrSold - GarageYrBlt)) %>%
  
  # removes the columns used in above the calculations
  dplyr::select(!c(Id,MSSubClass, MiscVal, YrSold , 
                   MoSold, YearBuilt, YearRemodAdd))


# Convert all character data to factor
hd[sapply(hd, is.character)] <- 
  lapply(hd[sapply(hd, is.character)], as.factor)
```

### Impute Missing Values with `PMM`

Make data set of `numeric` variables
```{r}
hd.numericRaw <- hd %>%
  
  #selecting all the numeric data
  dplyr::select_if(is.numeric) %>%
  
  #converting the data frame to tibble
  as_tibble() 
```

Make data set of `factor` variables
```{r}
hd.factorRaw <- hd %>%
  
  #selecting all the numeric data
  dplyr::select_if(is.factor) %>%
  
  #converting the data frame to tibble
  as_tibble()
```


For each column with missing data, impute missing values with `PMM`

* Done with function `imputeWithPMM()` function  
* Applies function via `dplyr` logic  
* Note `seeImputation()` function to visualize the imputation from prior homework 4, not shown for simplicity in viewing


Create function to impute via `PMM`

```{r}
imputeWithPMM <- function(colWithMissingData) {
  
  # Using the mice package
  suppressMessages(library(mice))
  
  # Discover the missing rows
  isMissing <- is.na(colWithMissingData) 
  
  # Create data frame to pass to PMM imputation function from mic package
  df <- data.frame(x       = rexp(length(colWithMissingData)), # meaningless x to help show variation 
                   y       = colWithMissingData, 
                   missing = isMissing)
  
  # imputation by PMM
  df[isMissing, "y"] <- mice.impute.pmm( df$y, 
                                        !df$missing, 
                                         df$x)
  
  return(df$y)
}
```


```{r, include=FALSE}
# Note `see Imputation()` function to visualize the imputation from prior homework 4
seeImputation <- function(df, df.meanInputed, 
                          imputationMethod) {
  
  # Min/Max ranges so actual and imputed histograms align
  yMin = quantile(df.meanInputed$y, 0.05)
  yMax = max(df.meanInputed$y)
  
  # Non Altered data -------------------------------------------------
  
  meanVal = mean(df$y, na.rm=T) # mean of the non altered data
  
  # Create the plot
  p1 <- df %>%
    ggplot(aes(x = y)) +
    
    # Histogram
    geom_histogram(color = 'grey65', fill = 'grey95') +
    
    # The mean value line
    geom_vline(xintercept = meanVal, color = 'tomato3') +
    
    # Text associated with mean value
    annotate("text", 
             label = "Mean Value", 
             x = meanVal, y = 100, 
             size = 5, colour = "tomato3" ) +
    
    # Labels
    labs(title = 'Data with Missing Values',
         y     = 'Frequency', 
         x     = '' ) +
    
    xlim(yMin, yMax) + # min and max range of x axis (for equal comparison)
    theme_minimal() # Theme
  
  
  # Imputed data -------------------------------------------------

  meanValImpute = mean(df.meanInputed$y, na.rm=T)
  
  # Create the plot
  p2 <- df.meanInputed %>%
    ggplot(aes(x = y)) +
    
    # Histogram
    geom_histogram(color = 'grey65', fill = 'grey95') +
    
    # The mean value line
    geom_vline(xintercept = meanVal, color = 'tomato3') +
    
    # Text associated with mean value
    annotate("text", 
             label = "Mean Value", 
             x = meanValImpute, y = 100, 
             size = 5, colour = "tomato3" ) +
    
    # Labels
    labs(title = 'Data without Missing Values',
             subtitle = 'Using PMM',
             y = 'Frequency', 
             x = imputationMethod) +
    
    xlim(yMin, yMax) + # min and max range of x axis (for equal comparison)
    theme_minimal() # Theme
  
  # Variation scatter ----------------------------------------------------------
  
  p3 <- df.meanInputed %>% ggplot(aes(x=rexp(length(y)), y=y, color=is.na(df$y))) + 
    
    # Add points
    geom_point(alpha = 0.5) +
    
    # Colors, limits, labels, and themes
    scale_color_manual(values = c('grey80', 'tomato3'),
                       labels = c('Actuals', 'Imputed') ) +
    ylim(0, quantile(df.meanInputed$y, 0.99)) + # lower 99% of dist
    labs(title   = 'Variation of Actuals vs. Imputed Data',
         x       = 'x', 
         y       = imputationMethod,
         caption =paste0('\nUsing housing.csv data',
                         '\nOnly showing lower 99% of distribution for viewing') 
         ) +
    theme_minimal() + theme(legend.position = 'bottom',
                            legend.title    = element_blank())
  
  
  # Combine the plots for the final returned output
  combinedPlots <- plot_grid(p1, p2, p3, 
                             ncol = 1, label_size = 12,
                             rel_heights = c(1, 1.1, 1.75))
  return(combinedPlots)
}
```


Apply `PMM` function to numeric data containing null values 
```{r, warning=FALSE, message=FALSE}
# Data to store imputed values with PMM method
hd.Imputed <- hd

# Which columns has Na's?
colNamesWithNulls <- colnames(hd.numericRaw[ , colSums(is.na(hd.numericRaw)) != 0])
colNamesWithNulls

numberOfColsWithNulls = length(colNamesWithNulls)


# For each of the numeric columns with null values
for (colWithNullsNum in 1:numberOfColsWithNulls) {
  
  # The name of the column with null values
  nameOfThisColumn <- colNamesWithNulls[colWithNullsNum]
  
  # Get the actual data of the column with nulls
  colWithNulls <- hd[, nameOfThisColumn]
  
  # Impute the missing values with PMM
  imputedValues <- imputeWithPMM(colWithNulls)
  
  # Now store the data in the original new frame
  hd.Imputed[, nameOfThisColumn] <- imputedValues
  
  # Save a visualization of the imputation
  pmmVisual <- seeImputation(data.frame(y = colWithNulls),
                             data.frame(y = imputedValues),
                             nameOfThisColumn )
  
  fileToSave = paste0('OutputPMM/Imputation_With_PMM_', nameOfThisColumn, '.pdf')
  print(paste0('For imputation results of ', nameOfThisColumn, ', see ', fileToSave))
  ggsave(pmmVisual, filename = fileToSave,
         height = 11, width = 8.5)
}
```

\newpage

### Factor Level Collapse - Create `Other` Bin for Columns over `4` Unique Values 
```{r}
hd.Cleaned <- hd.Imputed # For final cleaned data

# Get list of factors and the number of unique values
factorCols <- as.data.frame(t(hd.factorRaw %>% summarise_all(n_distinct)))

# We are going to factor collapse factor columns with more than 4 columns
# So there will be 4 of the original, and 1 containing 'other'
# This is the threshold
factorThreshold = 4

# Get a list of the factors we are going to collapse
colsWithManyFactors <- rownames(factorCols %>% filter(V1 > factorThreshold))

# Show a summary of how many factors will be collapsed
numberOfColsWithManyFactors = length(colsWithManyFactors)
paste('Before cleaning, there are', numberOfColsWithManyFactors, 'factor columns with more than', 
      factorThreshold, 'unique values')

# Collapse the affected factors in the original data (the one that already has imputation)

## for each factor column that we are about to collapse
for (collapsedColNum in 1:numberOfColsWithManyFactors) {
  
  # The name of the column with null values
  nameOfThisColumn <- colsWithManyFactors[collapsedColNum]
  
  # Get the actual data of the column with nulls
  colWithManyFactors <- hd[, nameOfThisColumn]
  
  # lumps all levels except for the n most frequent 
  hd.Cleaned[, nameOfThisColumn] <- fct_lump_n(colWithManyFactors, 
                                                       n=factorThreshold)
}

# Check to see if the factor lumping worked
factorColsCleaned <- t(hd.Cleaned %>% 
                       select_if(is.factor) %>%
                       summarise_all(n_distinct))
paste('After cleaning, there are', sum(factorColsCleaned > factorThreshold, na.rm = TRUE), 
      "columns with more than", factorThreshold, "unique values (omitting NA's)")
```

\newpage

### Remove Outliers from Numeric Data
* Since there are so many outliers, we are only going to remove some outliers  
* If you count the number of outliers by column, the 75% of columns contain less
than 50 outliers.  
* However, some contain up to 200. Since remove ALL outliers would reduce the size of the data to 
less than 300 observations, we are removing up to 50 per column.


```{r}
hd.CleanedNoOutliers <- hd.Cleaned

# Remove up to 75% of the outliers in the data set
# this is the 3rd quartile of number of outliers.
k_outliers = 50 
numOutliers = c() # to store the number of outliers per column

theColNames <- colnames(hd.Cleaned)

for (colNum in 1:ncol(hd.Cleaned)) {

  theCol <- hd.Cleaned[, colNum]
  nrowBefore = length(theCol)
  colName <- theColNames[colNum]
  

  # Only consider numeric
  if (is.numeric(theCol)) {
        
    # Identify the outliers in the column
    # Source: https://www.geeksforgeeks.org/remove-outliers-from-data-set-in-r/
    columnOutliers <- boxplot.stats(hd.CleanedNoOutliers[, colNum])$out
    numOutliers <- c(numOutliers, length(columnOutliers))
    
    # Now remove k outliers from the column
    if (length(columnOutliers) < k_outliers) {
      
      hd.CleanedNoOutliers  <- hd.CleanedNoOutliers %>%
        
        # If this syntax looks weird, it is just referencing a column in the 
        # data set using dplyr piping. See below for more info:
        # https://stackoverflow.com/questions/48062213/dplyr-using-column-names-as-function-arguments
        # https://stackoverflow.com/questions/72673381/column-names-as-variables-in-dplyr-select-v-filter
        filter( !( get({{colName}}) %in% columnOutliers ) )
    }
  }
}
paste0('Of the columns with outliers, removed up to 75th percentile of num. outliers.')
paste0('See that the 75th percentile of columns with outliers contain ',
       paste0(summary(numOutliers)[5]), ' outliers')
```


\newpage

# `1 (a)` - OLS Model


\newpage

# `1 (b)` - PLS Model


\newpage

# `1 (c)` - LASSO Model


\newpage

# `1 (d)` - Model Variants

## `1 (d, i)` - PCR Model

### Model Setup
* Uses `numeric` data for Principal Component Analysis  
* Then appends the `factor` data to the data *without `NULL` values*   
* Finally, uses `stepAIC()` to best model data  
* See interpretation at end

Get cleaned `numeric` and `factor` `data frames`
```{r}
# After cleaning, two data sets that contain..

## Numeric data ---------------------------------------------------
hd.numericClean <- hd.Cleaned %>% select_if(is.numeric)

## Factors -------------------------------------------------------
hd.factorClean  <- hd.Cleaned %>% dplyr::select(where(is.factor))

# Removing any columns with NA
removeColsWithNA <- function(df) {
  return( df[ , colSums(is.na(df)) == 0] )
}
hd.factorClean <- removeColsWithNA(hd.factorClean)

paste('Num. factor cols. removed due to null values:', 
      ncol(hd.Cleaned %>% dplyr::select(where(is.factor)) ) - ncol(hd.factorClean) )
paste(ncol(hd.factorClean), 'factor cols. remain') 
```

Perform PCA
```{r}
# Principal component analysis on numeric data
pc.house <- prcomp(hd.numericClean %>% dplyr::select(-SalePrice), # do not include response var
                   center = TRUE, # Mean centered  
                   scale  = TRUE  # Z-Score standardized
                   )

# See first 10 cumulative proportions
pc.house.summary <- summary(pc.house)
pc.house.summary$importance[, 1:10]
```

Now we choose number of PC's that explain 75% of the variation

* Note this threshold is just a judgement call. No significance behind 75%

```{r}
cumPropThreshold = 0.75 # The threshold

numPCs <- sum(pc.house.summary$importance['Cumulative Proportion', ] < cumPropThreshold)
paste0('There are ', numPCs, ' principal components that explain up to ', cumPropThreshold*100, 
       '% of the variation in the data')

chosenPCs <- as.data.frame(pc.house$x[, 1:numPCs])
```

Join on the factor data
```{r}
df.pcr <- cbind(SalePrice = hd.numericClean$SalePrice, chosenPCs, hd.factorClean) 
```


### Fit the Model
* Linear model containing:  
  - Principal components explaining 75% of variation in `numeric` data
  - Non-null `factor` data  
  - *Predicted variable:* `log(SalePrice)`

* Then use `stepAIC()` to identify which variables are actually important for model 
  
```{r pcr, echo=T, results='hide'}
# Fit data using PC's, non-null factors
fit.pcr <- lm(log(SalePrice) ~ ., data = df.pcr)

# Reduce to only important variables
fit.pcrReduced <- stepAIC(fit.pcr, direction="both")
```

```{r, echo=FALSE}
# Key diagnostics for final summary table
est.pcr <- summary(fit.pcrReduced)

# Get the RMSE and R Squared of the model
keyDiagnostics.pcr <- data.frame(Model    = 'PCR',
                                 Notes    = 'lm',
                                 Hyperparameters = 'N/A',
                                 RMSE     = sqrt(mean(fit.pcrReduced$residuals^2)),
                                 Rsquared = est.pcr$adj.r.squared)

# Show output
keyDiagnostics.pcr %>% kable()
```


```{r}
# View results
summary(fit.pcrReduced)
```

### View and Interpret Results

*Please note all interpretations below are approximate, given the `stepAIC()` uses stochastic modeling.*

**Model performance evaluation:**  

* See that around 28 of the variables cannot be explained by random chance, 
with a probability of 90% or more (see significance codes above)   

* Standard errors range from $\pm$ 1-5%, with average around 2%. Larger values may indicate higher 
uncertainty of the estimated coefficients.  

* This model explains around 92% of the variation in the `log(SalePrice)`. See Adjusted R-Squared for reference.  

* Note this model may exhibit selection bias, since the data excludes factor data 
with null values in the variable.  

* This model would likely doe well for prediction of `log(SalePrice)`, given the small 
range of standard errors, high adjusted R squared, and number of significant variables.
This model would obviously not do well for inference, given we are using principal
components that mask the numeric data.  


**Practical significance evaluation:**  

* The principal components contribute positively about 20% of the sale price of the home  

* Residential Medium Density (`MSZoningRM`) reduces the home price by around 12%,
with a standard error of around 2%.

* If the exterior quality is below average (`ExterQualBelowAvg`), it reduces the home price by around 12%,
with a standard error of around 5%.

* If the functionality of the home has 2 major deductions (`FunctionalMaj2`), 
it reduces the home price by around 20%, with a standard error of around 6%. 
While having typical functionality (`FunctionalTyp`) increases the home sale price 
by nearly 10%, with a standard error of 3%.  

* See other coefficients of the data for other variables.

\newpage

View Predicted vs. Actuals

Function to compare predicted vs. observed values
```{r}
# Function to compare predicted vs. actual (observed) regression outputs
predictedVsObserved <- function(predicted, observed, modelName, outcomeName = 'Log(SalePrice)') {
  
  ## Create data set for predicted vs. actuals
  comparison <- data.frame(observed  = observed,
                           predicted = predicted) %>%
    
    # Row index
    mutate(ID = row_number()) %>%
  
    # Put in single column
    pivot_longer(cols      = c('observed', 'predicted'),
                 names_to  = 'metric',
                 values_to = 'value') 
  
  
  # Plot --- Observed vs. Actuals across all variables in data
  variationScatter <- comparison %>%
    ggplot(aes(x     = ID,
               y     = value,
               color = metric
               )
           ) +
    geom_point(alpha = 0.5, size = 1) + 
    
    labs(title = 'Variation in Predicted vs. Observed Data',
         subtitle = paste('Model:', modelName),
         x = 'X', y = outcomeName) + 
    theme_minimal() + theme(legend.title = element_blank(),
                            legend.position = 'top') +
    scale_color_manual(values = c('grey60', 'palegreen3'))
  
  
  print(variationScatter)
  
  # Limit for x and y axis for scatter of predicted vs. observed
  axisLim = c( min(c(predicted, observed)), max(c(predicted, observed)) )
  
  
  # Simple comparison of data
  plot(x = observed,
       y = predicted,
       main = paste(modelName, 'Model - Actual (Observed) vs. Predicted\n'),
       xlab = paste('Observed Values -', outcomeName),
       ylab = paste('Predicted Values -', outcomeName),
       pch  = 16,
       cex  = 0.75,
       col  = alpha('steelblue3', 1/4),
       xlim = axisLim,
       ylim = axisLim
  )
  
  # Add the Predicted vs. actual line
  abline(lm(predicted ~ observed), col = 'steelblue3', lwd = 2)
  mtext('Predicted ~ Actual', side = 3, adj = 1, col = 'steelblue3')
  
  # Add line for perfectly fit model
  abline(0,1, col = alpha('tomato3', 0.8), lwd = 2)
  mtext('Perfectly Fit Model', side = 1, adj = 0, col = 'tomato3')
}
```


View results of the PCR Model  

* See that the variation in the data is very closely resembled actual by changes in independent variables

* Implication? This model fits its own data well, but what is not know if it can predict
out of sample data.  

* Note that it the data (blue) deviates slightly from perfect line model (red), indicating
that the model is slightly skewed from predicted and actual data.

```{r}
# How do the predicted vs. Actuals Compare?
predictedVsObserved(observed  = log(df.pcr$SalePrice),
                    predicted = predict(fit.pcrReduced),
                    modelName = 'PCR')
```

\newpage

## `1 (d, ii)` - SVR Model

### Model Setup
```{r}
ctrl <- trainControl(method  = "repeatedcv", 
                     number  = 5, # 5 fold cross validation
                     repeats = 2  # 5 repeats
                     )

# The data (PMM imputed values, and only whole columns without NA. Does not omit outliers)
df.svm <- cbind(SalePrice = hd.numericClean$SalePrice, 
                hd.numericClean, hd.factorClean) 
```

### Fit the Model
```{r svm, warning=FALSE, message=FALSE}
# Train and tune the SVM
fit.svm <- train(data = df.svm, 
                 log(SalePrice) ~ .,
                 method     = "svmRadial",         # Radial kernel
                 tuneLength = 9,                   # 9 values of the cost function
                 preProc    = c("center","scale"), # Center and scale data
                 trControl  = ctrl)
```

### View and Interpret Results
* Note all numbers mentioned below are approximate
* See that the R Squared of the model is around 0.86, and RMSE is 0.14
* See that the model predicts the data well. 
* Also, note that the model predicts the data with less error than the linear model. 
See this from the RMSE or scatter plot of predicted values.

```{r, echo=FALSE}
# Gather key diagnostics for summary table
# Get the RMSE and R Squared of the model
hyperparameters.svm = list('C' = fit.svm[["finalModel"]]@param[["C"]],
                           'Epsilon' = fit.svm[["finalModel"]]@param[["epsilon"]])


keyDiagnostics.svm <- data.frame(Model    = 'SVM',
                                 Notes    = 'caret and svmRadial',
                                 Hyperparameters = paste('C =', hyperparameters.svm$C, ',',
                                                         'Epsilon =', hyperparameters.svm$Epsilon)
                                 )

keyDiagnostics.svm <- cbind(keyDiagnostics.svm,
                            fit.svm$results %>% 
                              filter(C == hyperparameters.svm$C) %>%
                              dplyr::select(RMSE, Rsquared)
                      )

# Show output
keyDiagnostics.svm %>% kable()
```


```{r, fig.height=4}
# Final model?
fit.svm$finalModel

# How do the predicted vs. Actuals Compare?
predictedVsObserved(observed  = log(df.svm$SalePrice),
                    predicted = predict(fit.svm, df.svm),
                    modelName = 'SVM')
```


\newpage

## `1 (d, iii)` - MARS Model

### Fit the Model
```{r}
# Train and tune the MARS model
fit.mars <- train(data = df.svm, # note this is fine since data is the same for this model
                  log(SalePrice) ~ .,
                  method     = "earth",             # Radial kernel
                  tuneLength = 9,                   # 9 values of the cost function
                  preProc    = c("center","scale"), # Center and scale data
                  trControl  = ctrl
                 )
```

```{r, echo=FALSE}
# Key diagnostics for final model

# Get the RMSE and R Squared of the model
hyperparameters.mars = list('degree' = fit.mars[["bestTune"]][["degree"]],
                            'nprune' = fit.mars[["bestTune"]][["nprune"]])

keyDiagnostics.mars <- data.frame(Model   = 'MARS',
                                  Notes    = 'caret and earth',
                                  Hyperparameters = paste('Degree =', hyperparameters.mars$degree, ',',
                                                          'nprune =', hyperparameters.mars$nprune)
                                  )

keyDiagnostics.mars <- cbind(keyDiagnostics.mars,
                            fit.mars$results %>% 
                              filter(degree == hyperparameters.mars$degree,
                                     nprune == hyperparameters.mars$nprune) %>%
                              dplyr::select(RMSE, Rsquared)
                      )

# Show output
keyDiagnostics.mars %>% kable()
```

### View and Interpret Results
* See that the model overall performs very well, and in fact performs
similarly to the PCR model (in terms of RMSE and Adjusted R Squared).  
* Again, unsure if the model would do well for prediction of out of sample data,
but fits this data extremely well.  

```{r, fig.height=4}
# Final model?
fit.mars$finalModel

# How do the predicted vs. Actuals Compare?
predicted.mars = fit.mars[["finalModel"]][["fitted.values"]]
colnames(predicted.mars) <- 'predicted'

predictedVsObserved(observed  = log(df.svm$SalePrice),
                    predicted = predicted.mars,
                    modelName = 'MARS')
```



\newpage

# Summary Table of Model Performance
```{r, echo=FALSE}
# Add the key diagnostics here
rbind(keyDiagnostics.pcr, 
      keyDiagnostics.svm, 
      keyDiagnostics.mars
      ) %>%
  
  # Round to 4 digits across numeric data
  mutate_if(is.numeric, round, digits = 4) %>%
  
  # Spit out kable table
  kable()
```
