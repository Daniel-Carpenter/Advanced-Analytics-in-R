---
title: "Rough"
author: "Sonaxy Mohanty"
date: "10/3/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages
```{r, warning=FALSE, error=FALSE}
library(tidyverse)
library(mice)
library(cowplot)
library(ggplot2)
library(GGally) ##ggcorr function
library(caTools) #hold-out validation
library(MASS)
library(regclass)
library(Metrics) #RMSE calculation
library(broom) #get the p-value of  the model
library(car) #ncvTest function
```
  
## General Data Prep

### Read Data
```{r}
# Convert all character data to factor 
hd <- read.csv('housingData.csv', stringsAsFactors = TRUE) %>%

# creates new variables age, ageSinceRemodel, and ageofGarage and
  dplyr::mutate(age = YrSold - YearBuilt,
                ageSinceRemodel = YrSold - YearRemodAdd,
                ageofGarage = ifelse(is.na(GarageYrBlt), age, YrSold - GarageYrBlt)) %>%

# remove some columns used in the above calculations
  dplyr::select(!c(Id,YrSold , 
                   MoSold, YearBuilt, YearRemodAdd))


#str(hd)
```  

### Impute Missing Values with `PMM`

Make data set of `numeric` variables
```{r}
hd.numericRaw <- hd %>%
  
  #selecting all the numeric data
  dplyr::select_if(is.numeric) %>%
  
  #converting the data frame to tibble
  as_tibble() 
```

Make data set of `factor` variables
```{r}
hd.factorRaw <- hd %>%
  
  #selecting all the numeric data
  dplyr::select_if(is.factor) %>%
  
  #converting the data frame to tibble
  as_tibble()
```  
  
For each column with missing data, impute missing values with `PMM`

* Done with function `imputeWithPMM()` function  
* Applies function via `dplyr` logic  
* Note `seeImputation()` function to visualize the imputation from prior homework 4, not shown for simplicity in viewing  
  
Create function to impute via `PMM`  
```{r}
imputeWithPMM <- function(colWithMissingData) {
  
  # Using the mice package
  #suppressMessages(library(mice))
  #?suppressMessages
  # Discover the missing rows
  isMissing <- is.na(colWithMissingData) 
  
  # Create data frame to pass to PMM imputation function from mic package
  df <- data.frame(x       = rexp(length(colWithMissingData)), # meaningless x to help show variation 
                   y       = colWithMissingData, 
                   missing = isMissing)
  
  # imputation by PMM
  df[isMissing, "y"] <- mice.impute.pmm( df$y, 
                                        !df$missing, 
                                         df$x)
  
  return(df$y)
}
```

```{r, include=FALSE}
# Note `see Imputation()` function to visualize the imputation from prior homework 4
seeImputation <- function(df, df.meanInputed, 
                          imputationMethod) {
  
  # Min/Max ranges so actual and imputed histograms align
  yMin = quantile(df.meanInputed$y, 0.05)
  yMax = max(df.meanInputed$y)
 
  # Non Altered data -------------------------------------------------
  
  meanVal = mean(df$y, na.rm=T) # mean of the non altered data
  
  # Create the plot
  p1 <- df %>%
    ggplot(aes(x = y)) +
    
    # Histogram
    geom_histogram(color = 'grey65', fill = 'grey95') +
    
    # The mean value line
    geom_vline(xintercept = meanVal, color = 'tomato3') +
    
    # Text associated with mean value
    annotate("text", 
             label = "Mean Value", 
             x = meanVal, y = 100, 
             size = 5, colour = "tomato3" ) +
    
    # Labels
    labs(title = 'Data with Missing Values',
         y     = 'Frequency', 
         x     = '' ) +
    
    xlim(yMin, yMax) + # min and max range of x axis (for equal comparison)
    theme_minimal() # Theme
  
  
  # Imputed data -------------------------------------------------

  meanValImpute = mean(df.meanInputed$y, na.rm=T)
  
  # Create the plot
  p2 <- df.meanInputed %>%
    ggplot(aes(x = y)) +
    
    # Histogram
    geom_histogram(color = 'grey65', fill = 'grey95') +
    
    # The mean value line
    geom_vline(xintercept = meanVal, color = 'tomato3') +
    
    # Text associated with mean value
    annotate("text", 
             label = "Mean Value", 
             x = meanValImpute, y = 100, 
             size = 5, colour = "tomato3" ) +
    
    # Labels
    labs(title = 'Data without Missing Values',
             subtitle = 'Using PMM',
             y = 'Frequency', 
             x = imputationMethod) +
    
    xlim(yMin, yMax) + # min and max range of x axis (for equal comparison)
    theme_minimal() # Theme
  
  # Variation scatter ----------------------------------------------------------
  
  p3 <- df.meanInputed %>% ggplot(aes(x=rexp(length(y)), y=y, color=is.na(df$y))) + 
    
    # Add points
    geom_point(alpha = 0.5) +
    
    # Colors, limits, labels, and themes
    scale_color_manual(values = c('grey80', 'tomato3'),
                       labels = c('Actuals', 'Imputed') ) +
    ylim(0, quantile(df.meanInputed$y, 0.99)) + # lower 99% of dist
    labs(title   = 'Variation of Actuals vs. Imputed Data',
         x       = 'x', 
         y       = imputationMethod,
         caption =paste0('\nUsing housing.csv data',
                         '\nOnly showing lower 99% of distribution for viewing') 
         ) +
    theme_minimal() + theme(legend.position = 'bottom',
                            legend.title    = element_blank())
  
  
  # Combine the plots for the final returned output
  combinedPlots <- plot_grid(p1, p2, p3, 
                             ncol = 1, label_size = 12,
                             rel_heights = c(1, 1.1, 1.75))
  return(combinedPlots)
}
```
  
Apply `PMM` function to numeric data containing null values 
```{r, warning=FALSE, message=FALSE}
# Data to store imputed values with PMM method
hd.Imputed <- hd

# Which columns has Na's?
colNamesWithNulls <- colnames(hd.numericRaw[ , colSums(is.na(hd.numericRaw)) != 0])
colNamesWithNulls

numberOfColsWithNulls = length(colNamesWithNulls)


# For each of the numeric columns with null values
for (colWithNullsNum in 1:numberOfColsWithNulls) {
  
  # The name of the column with null values
  nameOfThisColumn <- colNamesWithNulls[colWithNullsNum]
  
  # Get the actual data of the column with nulls
  colWithNulls <- hd[, nameOfThisColumn]
  
  # Impute the missing values with PMM
  imputedValues <- imputeWithPMM(colWithNulls)
  
  # Now store the data in the original new frame
  hd.Imputed[, nameOfThisColumn] <- imputedValues
  
  # Save a visualization of the imputation
  pmmVisual <- seeImputation(data.frame(y = colWithNulls),
                             data.frame(y = imputedValues),
                             nameOfThisColumn )

  fileToSave = paste0('OutputPMM/Imputation_With_PMM_', nameOfThisColumn, '.pdf')
  print(paste0('For imputation results of ', nameOfThisColumn, ', see ', fileToSave))
  dir.create("OutputPMM/")
  ggsave(pmmVisual, filename = fileToSave,
         height = 11, width = 8.5)
  
  #hd.Imputed[!complete.cases(is.numeric(hd.Imputed)), ]

}
```
  
\newpage

### Factor Level Collapse - Create `Other` Bin for Columns over `4` Unique Values 
```{r}
hd.Cleaned <- hd.Imputed # For final cleaned data

# Get list of factors and the number of unique values
factorCols <- as.data.frame(t(hd.factorRaw %>% summarise_all(n_distinct)))

# We are going to factor collapse factor columns with more than 4 columns
# So there will be 4 of the original, and 1 containing 'other'
# This is the threshold
factorThreshold = 4

# Get a list of the factors we are going to collapse
colsWithManyFactors <- rownames(factorCols %>% filter(V1 > factorThreshold))

# Show a summary of how many factors will be collapsed
numberOfColsWithManyFactors = length(colsWithManyFactors)
paste('Before cleaning, there are', numberOfColsWithManyFactors, 'factor columns with more than', 
      factorThreshold, 'unique values')

# Collapse the affected factors in the original data (the one that already has imputation)

## for each factor column that we are about to collapse
for (collapsedColNum in 1:numberOfColsWithManyFactors) {
  
  # The name of the column with null values
  nameOfThisColumn <- colsWithManyFactors[collapsedColNum]
  
  # Get the actual data of the column with nulls
  colWithManyFactors <- hd[, nameOfThisColumn]
  
  # lumps all levels except for the n most frequent 
  hd.Cleaned[, nameOfThisColumn] <- fct_lump_n(colWithManyFactors, 
                                                       n=factorThreshold)
}

# Check to see if the factor lumping worked
factorColsCleaned <- t(hd.Cleaned %>% 
                       select_if(is.factor) %>%
                       summarise_all(n_distinct))
paste('After cleaning, there are', sum(factorColsCleaned > factorThreshold, na.rm = TRUE), 
      "columns with more than", factorThreshold, "unique values (omitting NA's)")
```
  
\newpage

### Remove Outliers from Numeric Data
* Since there are so many outliers, we are only going to remove some outliers  
* If you count the number of outliers by column, the 75% of columns contain less
than 50 outliers.  
* However, some contain up to 200. Since remove ALL outliers would reduce the size of the data to 
less than 300 observations, we are removing up to 50 per column.


```{r}
hd.CleanedNoOutliers <- hd.Cleaned

# Remove up to 75% of the outliers in the data set
# this is the 3rd quartile of number of outliers.
k_outliers = 50 
numOutliers = c() # to store the number of outliers per column

theColNames <- colnames(hd.Cleaned)

for (colNum in 1:ncol(hd.Cleaned)) {

  theCol <- hd.Cleaned[, colNum]
  nrowBefore = length(theCol)
  colName <- theColNames[colNum]
  

  # Only consider numeric
  if (is.numeric(theCol)) {
        
    # Identify the outliers in the column
    # Source: https://www.geeksforgeeks.org/remove-outliers-from-data-set-in-r/
    columnOutliers <- boxplot.stats(hd.CleanedNoOutliers[, colNum])$out
    numOutliers <- c(numOutliers, length(columnOutliers))
    
    # Now remove k outliers from the column
    if (length(columnOutliers) < k_outliers) {
      
      hd.CleanedNoOutliers  <- hd.CleanedNoOutliers %>%
        
        # If this syntax looks weird, it is just referencing a column in the 
        # data set using dplyr piping. See below for more info:
        # https://stackoverflow.com/questions/48062213/dplyr-using-column-names-as-function-arguments
        # https://stackoverflow.com/questions/72673381/column-names-as-variables-in-dplyr-select-v-filter
        filter( !( get({{colName}}) %in% columnOutliers ) )
    }
  }
}
paste0('Of the columns with outliers, removed up to 75th percentile of num. outliers.')
paste0('See that the 75th percentile of columns with outliers contain ',
       paste0(summary(numOutliers)[5]), ' outliers')
```

\newpage  
  
## Exploratory Data Analysis

### Checking the distribution of Sale Price of houses
  
```{r}
hist(hd.CleanedNoOutliers$SalePrice, 
     col = 'skyblue4',
     main = 'Distribution of Sale Price of houses',
     xlab = 'House Price')

```
  
* After removing the desired outliers, we can see that the distribution of Sale Price looks like a normal distribution with few outliers on the right tail.  
  
### Correlation between features in the dataset  
  
```{r, warning=FALSE, fig.height=6}
ggcorr(hd.CleanedNoOutliers, geom='blank', label=T, label_size=3, hjust=1,
       size=3, layout.exp=2) +
  geom_point(size = 4, aes(color = coefficient > 0, alpha = abs(coefficient) >= 0.5)) +
  scale_alpha_manual(values = c("TRUE" = 0.25, "FALSE" = 0)) +
  guides(color = F, alpha = F)

```
  
* We can see that `SalePrice` has strong correlations with `GarageArea`, `GarageCars`, `TotRmsAbvGrd`, `FullBath`, `GrLivArea`, `X1stFlrSF`, `TotalBsmtSF`, `OverallQual`.  

# `1 (a)` - OLS Model  
  
## `i`.  

### Hold-out validation set  
  
* Since, we have deleted some of the outlier values during data pre-processing, using 10% of the data as test and remaining 90% as train
  
```{r}
idx <- sample(nrow(hd.CleanedNoOutliers), nrow(hd.CleanedNoOutliers)*0.1)
test <- hd.CleanedNoOutliers[idx,]
train <- hd.CleanedNoOutliers[-idx,]
```
 
### Fit the OLS Model  
  
`Model 1`:  
* Linear model containing:  
  - *Independent variables:* `GarageArea + GarageCars + TotRmsAbvGrd + FullBath + GrLivArea + X1stFlrSF + TotalBsmtSF + OverallQual`
  - *Predicted variable:* `SalePrice`

```{r, results='hide'}
olsMdl1 <- lm(SalePrice ~ GarageArea + GarageCars + TotRmsAbvGrd 
              + FullBath + GrLivArea + X1stFlrSF + TotalBsmtSF + OverallQual, data=train)
```  

```{r, echo=FALSE, results='hide'}
summary(olsMdl1)
```
  
```{r, echo=FALSE, results='hide'}
AIC(olsMdl1)
BIC(olsMdl1)

olsMdl1_RMSE <- rmse(actual=train$SalePrice, predicted=olsMdl1$fitted.values)
olsMdl1_RMSE
```    
  
```{r, results='hide'}
VIF(olsMdl1)
```
  
* **For Model 1**: Adjusted R-squared is `0.8153`, AIC is `16689.91` and BIC is `16735.88` and RMSE is `20995.73`.  
* Still trying to improve the existing model.  
* No multicolinearity detected.  
  
`Model 2`:  
* Linear model containing:  
  - *Independent variables:* `GarageArea * GarageCars * TotRmsAbvGrd * FullBath * GrLivArea * X1stFlrSF * TotalBsmtSF * OverallQual`  
  - *Predicted variable:* `SalePrice`
```{r, results='hide'}
olsMdl2 <- lm(SalePrice ~ GarageArea * GarageCars * TotRmsAbvGrd 
              * FullBath * GrLivArea * X1stFlrSF * TotalBsmtSF * OverallQual, data=train)
```  
  
```{r, echo=FALSE, results='hide'}
summary(olsMdl2)
```  
  
```{r, results='hide'}
AIC(olsMdl2)
BIC(olsMdl2)

olsMdl2_RMSE <- rmse(actual=train$SalePrice, predicted=olsMdl2$fitted.values)
olsMdl2_RMSE

```  
  
* **For Model 2**: Adjusted R-squared is `0.8475`, AIC is `16737.27`, BIC is `17914.13` and RMSE is `15502.76`.  
* This model works better than the previous one.  
  
* The next model created is based on `Principal Component Analysis`.  
    - Uses `numeric` data for Principal Component Analysis  
    - Then appends the `factor` data to the data *without `NULL` values*   
    - Finally, uses `stepAIC()` to best model data

`Model 3`:  
  
Get cleaned `numeric` and `factor` `data frames`
```{r}
# After cleaning, two data sets that contain..

## Numeric data ---------------------------------------------------
hd.numericClean <- train %>% select_if(is.numeric)

## Factors -------------------------------------------------------
hd.factorClean  <- train %>% dplyr::select(where(is.factor))

# Removing any columns with NA
removeColsWithNA <- function(df) {
  return( df[ , colSums(is.na(df)) == 0] )
}
hd.factorClean <- removeColsWithNA(hd.factorClean)

paste('Num. factor cols. removed due to null values:', 
      ncol(train %>% dplyr::select(where(is.factor)) ) - ncol(hd.factorClean) )
paste(ncol(hd.factorClean), 'factor cols. remain') 
```

Perform PCA
```{r}
# Principal component analysis on numeric data
#to remove zero variance columns from the dataset, using the apply expression, 
#setting variance not equal to zero
pc.house <- prcomp(hd.numericClean[ , which(apply(hd.numericClean, 2, var) != 0)] %>%
                     dplyr::select(-SalePrice), # do not include response var
                   center = TRUE, # Mean centered  
                   scale  = TRUE  # Z-Score standardized
                   )

# See first 10 cumulative proportions
pc.house.summary <- summary(pc.house)
pc.house.summary$importance[, 1:10]
```

Now we choose number of PC's that explain 75% of the variation

* Note this threshold is just a judgement call. No significance behind 75%

```{r}
cumPropThreshold = 0.75 # The threshold

numPCs <- sum(pc.house.summary$importance['Cumulative Proportion', ] < cumPropThreshold)
paste0('There are ', numPCs, ' principal components that explain up to ', cumPropThreshold*100, 
       '% of the variation in the data')

chosenPCs <- as.data.frame(pc.house$x[, 1:numPCs])
```

Join on the factor data
```{r}
df.ols <- cbind(SalePrice = hd.numericClean$SalePrice, chosenPCs, hd.factorClean) 
```


### Fit the Model
* Linear model containing:  
  - Principal components explaining 75% of variation in `numeric` data
  - Non-null `factor` data  
  - *Predicted variable:* `SalePrice`

* Then use `stepAIC()` to identify which variables are actually important for model 
  
```{r}
# Fit data using PC's, non-null factors
fit.ols <- lm(SalePrice ~ ., data = df.ols)

# Reduce to only important variables
olsMdl3 <- stepAIC(fit.ols, direction="both")
```

* Reporting `all the variables` of the best model (`Model 3`):  
  
**Coefficient estimates**:  
  
```{r, echo=FALSE}
# Reporting the variables for best model
olsMdl3.sum <- summary(olsMdl3)

# Coefficient estimates of the model
olsMdl3.sum$coefficients
```  
  
**p-values**:  

```{r, echo=FALSE}
# p-values of the model
glance(olsMdl3)$p.value
```
  
**Adjusted R-squared**:  
```{r, echo=FALSE}
olsMdl3.sum$adj.r.squared
```
  
**AIC**:  
```{r, echo=FALSE}
AIC(olsMdl3)
```  
  
**BIC**:  
```{r, echo=FALSE}
BIC(olsMdl3)
```  
  
**VIF**:  
```{r}
VIF(olsMdl3)
```  
  
**RMSE**:  

```{r, echo=FALSE}
olsMdl3_RMSE <- rmse(actual=df.ols$SalePrice, predicted=olsMdl3$fitted.values)
olsMdl3_RMSE
```
  
* So, we can say that using PCA followed by stepAIC the OLS regression model is better as compared to the other OLS models built.  
* There is also no multicolinearity found in the model as the VIF values are less than 10.  

```{r, echo=FALSE, results='hide'}
# Key diagnostics for OLS: lm final summary table
olsMdl2.sum <- summary(olsMdl2)

# Get the RMSE and R Squared of the model
keyDiagnostics.olsMdl2 <- data.frame(Model    = 'OLS',
                                 Notes    = 'lm',
                                 Hyperparameters = 'N/A',
                                 RMSE     = olsMdl2_RMSE,
                                 Rsquared = olsMdl2.sum$adj.r.squared)

# Show output
keyDiagnostics.olsMdl2 %>% 
  knitr::kable()
```
  
```{r, echo=FALSE, results='hide'}
# Key diagnostics for OLS: lm + 2-way interactions final summary table

# Get the RMSE and R Squared of the model
keyDiagnostics.olsMdl3 <- data.frame(Model    = 'OLS',
                                 Notes    = 'lm + 2-way interactions',
                                 Hyperparameters = 'N/A',
                                 RMSE     = olsMdl3_RMSE,
                                 Rsquared = olsMdl3.sum$adj.r.squared)

# Show output
keyDiagnostics.olsMdl3 %>% 
  knitr::kable()
```  
  
## `ii`. Complete analysis of the residuals  
  
A linear regression model is considered fit if the below assumptions are met:  
  
* **Residuals should follow normal distribution**  
* **There should be no heteroscedasticity**  
* **There should be no multicollinearity**  

```{r}
hist(olsMdl3$residuals,
     col = 'skyblue4',
     main = 'Histogram of Residuals',
     xlab = 'Residuals')
```
  
We can see that the residuals are `normally distributed`.  
  
```{r}
plot(olsMdl3)
```
  
* From the *Residuals vs Fitted* plot, we can see there are points above and below the 0 line.  
* There is also a pattern seen like a `slight curvature pattern` which indicates that there maybe a systematic lack of fit.  
* From the *Normal Q-Q* plot, we can see that most of the points are `very close to the dotted line`, indicating that the residuals follow a normal distribution, except some points which might be outliers which maybe affecting the regression line fit of data.  
* Here the *Scale-Location* plot suggests that the red line is roughly horizontal across the plot and the spread of magnitude looks unequal, at some fitted values there are more residuals as compared to other like the ones in between 100000 and 150000, indicating some heteroskedasticity.  
* From the *Residuals vs Leverage* plot, we can see that there are no influential points in our regression model. We need to check `influencePlot` to see if we are missing any leverage.  
  
```{r}
influencePlot(olsMdl3)
```
 
* We can now see some high influential points for the fitted values - 741, 684, 712.  

```{r}
#ncv Test
ncvTest(olsMdl3)
```
  
Since `p-value` is less than significance level ($\alpha$) of `0.05`, that means we `reject the null hypothesis` of constant error variance which indicates heteroscedasticity.  
  
```{r}
VIF(olsMdl3)
```
  
Generally, VIF values which are greater than 5 or 7 are the cause of multicollinearity which we do not see in our model.  
  
**Improving the current model**:  
* To improve our model, we need to remove some influential observations from our model and then fit the regression model to the data.  
*  We can re-build the model with new predictors.  
* We can also perform variable transformation such as Box-Cox or use better evolved models like SVR, PCR etc., and see how it works.  

  

# References  
1. https://rpubs.com/staneaurelius/house_price_prediction  
