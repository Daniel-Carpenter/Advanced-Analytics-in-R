---
title: "Pricipal Component Analysis (PCA)"
author: "Daniel Carpenter"
format: 
  gfm:
    toc: true
    number-sections: true
    toc-depth: 2
standalone: true
    
editor: visual
---

Reading: Applied Predictive Modeling: 3.3

## Overview

-   Relies on highly correlated data. Some of the data in redundant

-   Goal is to maintain the same variance as the upper level dimension

### Data Requirements

Data must be:

-   Z-score standardized. Can mean center and Z-score scale with `prcomp(data, center = TRUE, scale = TRUE)`

-   Mean centered

-   ![](images/paste-3F88191E.png){width="781"}

### Example 1: Rotation of Data

#### Goes from 2D to 1D on highly correlated data

![](images/paste-D33882EE.png){width="400"}

![](images/paste-B128D915.png){width="400"}

## Math Behind PCA

+-------------------+------------------------------------------------------------------------------------------------+
| Variance          | ![](images/paste-BD5F3A4E.png){width="354"}                                                    |
+-------------------+------------------------------------------------------------------------------------------------+
| Covariance Matrix | ![](images/paste-E4390E88.png){width="450"}                                                    |
+-------------------+------------------------------------------------------------------------------------------------+
| Solution to PCA   | Make a judgement call on the % of variation that you contain.                                  |
|                   |                                                                                                |
|                   | -   Why do you have to make the call?                                                          |
|                   |                                                                                                |
|                   | -   Because maybe you need 2-3 variables for visualization, but 4 PC's are given.              |
|                   |                                                                                                |
|                   | -   Adjusting `q` could give you **less variables**, at the **expense of the representatives** |
|                   |                                                                                                |
|                   | ![](images/paste-76E5BAE4.png){width="450"}                                                    |
|                   |                                                                                                |
|                   | ![](images/paste-63633851.png){width="450"}                                                    |
+-------------------+------------------------------------------------------------------------------------------------+
| Normalization     | Each Attribute should contribute equally to the overall variance                               |
|                   |                                                                                                |
|                   | ![](images/paste-158F08ED.png){width="450"}                                                    |
+-------------------+------------------------------------------------------------------------------------------------+

## Conceptual Example of PCA

![](images/paste-925C48F1.png){width="550"}

### Two possible principal components:

![](images/paste-4D02DAD2.png){width="550"}

### How do the two PC's perform?

-   PC 1 explains 99% of the variance

-   PC 2 explains 00% of the variance

![](images/paste-C2DB5E9E.png){width="400"}

## PCA Example in `R`

```{r}
library(datasets)
library(scatterplot3d)
library(rgl)
library(ggplot2)


#to install ggbiplot, first install and load "devtools"
#then download ggbiplot from CRAN then load
#----------------------------------------------------------
# install.packages("devtools")
# library(devtools)
# install_github("vqv/ggbiplot")
library(ggbiplot)



x<-80*runif(1000)-40    #uniform random between -40 and 40
y<-40*runif(1000)-20    #uniform random between -20 and 40
z<-6*runif(1000)-3      #uniform random between -3 and 3



df<-data.frame(x,y,z)   #create df
df<-as.matrix(df)       #convert to matrix

scatterplot3d(df,xlim=c(-40,40),ylim=c(-40,40),zlim=c(-40,40))


#notice the axes views --
plot3d(df, col="red", size=4, xlim=c(-40,40),ylim=c(-40,40),zlim=c(-40,40))

#what do you expect the eigenvectors to be?
pc<-prcomp(df,center=T)
pc



summary(pc)
plot(pc)

str(pc)    #look at component pieces of the prcomp obj

# ?prcomp    #notice defaults regarding centering and scaling

pc$rotation

head(pc$x)


df2<-scale(df,scale=F)
mat<-df2%*%pc$rotation

mat[1:5,]


#for convenience:
newX<-pc$x[,1]
newY<-pc$x[,2]
newZ<-pc$x[,3]

scatterplot3d(newX,newZ,newY,xlim=c(-40,40),ylim=c(-40,40),zlim=c(-40,40),xlab="PC1",ylab="PC3",zlab="PC2")

#now the 2D plots

plot(newX,newY,xlim=c(-40,40),ylim=c(-40,40))          #plot x vs y
plot(newX,newZ,xlim=c(-40,40),ylim=c(-40,40))          #plot x vs z
plot(newY,newZ,xlim=c(-40,40),ylim=c(-40,40))          #plot y vs z


#now the biplots

biplot(pc)
ggbiplot(pc,circle=T,obs.scale=1,varname.size=20)
ggbiplot(pc,circle=T,choices=c(1,3),obs.scale=1,varname.size=20)





#re-run the above with the following inputs
x<-1*rnorm(1000)
y<-5*rnorm(1000)
z<-y+3*rnorm(1000)




#now for the example from lecture using the "state.x77" data

#let's see what we have to start with...

# ?state.x77

plot(state.x77[,8],state.x77[,2])


#PC with and without scaling...

prcomp(state.x77,scale=F)
prcomp(state.x77,scale=T)


#let's look at the results in more detail...


state.pca<-prcomp(state.x77,scale=T)

summary(state.pca)
plot(state.pca)

ggbiplot(state.pca)

ggbiplot(state.pca,obs.scale = 1, var.scale = 1, 
         varname.size = 4, labels.size=10, circle = TRUE)

#PC1 distinguishes between cold states with educated, harmless,
#long-lived populations, and warm, ill-educated, short-lived, violent states. \\

#PC2 distinguishes big rich educated
#states from small poor ignorant states, which tend to be a bit warmer,
#and less murderous.
```
